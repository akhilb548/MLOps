{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tourism Package Purchase Prediction - MLOps Project\n",
    "\n",
    "## Business Context\n",
    "\n",
    "\"Visit with us\" is a travel and tourism company that offers a variety of travel packages for destinations worldwide. The company's latest offering is the \"Wellness Tourism Package,\" designed to promote health and well-being through travel experiences.\n",
    "\n",
    "The marketing team has been running campaigns to promote this new package, but the conversion rate has been lower than expected. To improve targeting and increase conversions, the company wants to predict which customers are most likely to purchase the Wellness Tourism Package based on their demographics, travel preferences, and past interactions with the company.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Build an automated MLOps pipeline using GitHub Actions and Hugging Face to:\n",
    "- Register and prepare tourism customer data\n",
    "- Train multiple ML models with MLflow experiment tracking\n",
    "- Deploy the best model as a Streamlit web application\n",
    "- Automate the entire pipeline with CI/CD\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Create a GitHub Repository**\n",
    "   - Go to GitHub Profile ‚Üí Your repositories ‚Üí New\n",
    "   - Repository Name: `MLOps`\n",
    "   - Check the box for `README.md` file\n",
    "   - Click Create repository\n",
    "\n",
    "2. **Add Hugging Face Token to GitHub Secrets**\n",
    "   - Go to Hugging Face Profile ‚Üí Access Token\n",
    "   - Create New token (Write access)\n",
    "   - Copy the generated token\n",
    "   - In GitHub repo: Settings ‚Üí Secrets and Variables ‚Üí Actions\n",
    "   - Add Repository secret: Name = `HF_TOKEN`, Secret = (paste token)\n",
    "\n",
    "3. **Create Hugging Face Space**\n",
    "   - Go to Hugging Face ‚Üí Profile ‚Üí New Space\n",
    "   - Space name: `tourism-package-prediction-app`\n",
    "   - Select SDK: Docker\n",
    "   - Choose template: Streamlit\n",
    "   - Click Create Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"tourism_project\", exist_ok=True)\n",
    "print(\"Created tourism_project folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tourism_project/data\", exist_ok=True)\n",
    "print(\"Created tourism_project/data folder\")\n",
    "print(\"Please upload tourism.csv to the tourism_project/data folder before proceeding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/data_registration.py\n",
    "import os\n",
    "from huggingface_hub import HfApi, login\n",
    "import pandas as pd\n",
    "\n",
    "def register_dataset():\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged in to Hugging Face\")\n",
    "    else:\n",
    "        print(\"Warning: HF_TOKEN not found\")\n",
    "        return\n",
    "    \n",
    "    api = HfApi()\n",
    "    repo_id = \"tourism-package-prediction-data\"\n",
    "    repo_type = \"dataset\"\n",
    "    \n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_id, repo_type=repo_type, exist_ok=True, private=False)\n",
    "        print(f\"Repository '{repo_id}' created/verified\")\n",
    "        \n",
    "        dataset_path = \"tourism_project/data/tourism.csv\"\n",
    "        if os.path.exists(dataset_path):\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=dataset_path,\n",
    "                path_in_repo=\"tourism.csv\",\n",
    "                repo_id=repo_id,\n",
    "                repo_type=repo_type\n",
    "            )\n",
    "            print(f\"Dataset uploaded successfully to {repo_id}\")\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            print(f\"Dataset shape: {df.shape}\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "        else:\n",
    "            print(f\"Error: Dataset file not found at {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data registration: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    register_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/data_preparation.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from huggingface_hub import hf_hub_download, HfApi, login\n",
    "\n",
    "def load_data_from_hf():\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "    try:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=\"tourism-package-prediction-data\",\n",
    "            filename=\"tourism.csv\",\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded from Hugging Face, Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def clean_data(df):\n",
    "    df_clean = df.copy()\n",
    "    if 'Unnamed: 0' in df_clean.columns:\n",
    "        df_clean = df_clean.drop('Unnamed: 0', axis=1)\n",
    "    if 'CustomerID' in df_clean.columns:\n",
    "        df_clean = df_clean.drop('CustomerID', axis=1)\n",
    "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "    print(f\"Data cleaned, Final shape: {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "def encode_categorical_features(df):\n",
    "    df_encoded = df.copy()\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded '{col}': {len(le.classes_)} unique values\")\n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "def split_and_save_data(df):\n",
    "    X = df.drop('ProdTaken', axis=1)\n",
    "    y = df['ProdTaken']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    os.makedirs(\"tourism_project/data\", exist_ok=True)\n",
    "    train_path = \"tourism_project/data/train.csv\"\n",
    "    test_path = \"tourism_project/data/test.csv\"\n",
    "    train_data.to_csv(train_path, index=False)\n",
    "    test_data.to_csv(test_path, index=False)\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    return train_path, test_path\n",
    "\n",
    "def upload_to_hf(train_path, test_path):\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "    api = HfApi()\n",
    "    repo_id = \"tourism-package-prediction-data\"\n",
    "    try:\n",
    "        api.upload_file(path_or_fileobj=train_path, path_in_repo=\"train.csv\", repo_id=repo_id, repo_type=\"dataset\")\n",
    "        print(f\"Train data uploaded to {repo_id}\")\n",
    "        api.upload_file(path_or_fileobj=test_path, path_in_repo=\"test.csv\", repo_id=repo_id, repo_type=\"dataset\")\n",
    "        print(f\"Test data uploaded to {repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Hugging Face: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    print(\"DATA PREPARATION PIPELINE\")\n",
    "    df = load_data_from_hf()\n",
    "    df_clean = clean_data(df)\n",
    "    df_encoded, label_encoders = encode_categorical_features(df_clean)\n",
    "    train_path, test_path = split_and_save_data(df_encoded)\n",
    "    upload_to_hf(train_path, test_path)\n",
    "    print(\"DATA PREPARATION COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tourism_project/model_building\", exist_ok=True)\n",
    "print(\"Created tourism_project/model_building folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/model_building/model_training.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from huggingface_hub import hf_hub_download, HfApi, login\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_train_test_data():\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "    try:\n",
    "        train_path = hf_hub_download(repo_id=\"tourism-package-prediction-data\", filename=\"train.csv\", repo_type=\"dataset\")\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_path = hf_hub_download(repo_id=\"tourism-package-prediction-data\", filename=\"test.csv\", repo_type=\"dataset\")\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        X_train = train_df.drop('ProdTaken', axis=1)\n",
    "        y_train = train_df['ProdTaken']\n",
    "        X_test = test_df.drop('ProdTaken', axis=1)\n",
    "        y_test = test_df['ProdTaken']\n",
    "        print(f\"Data loaded - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    print(f\"{model_name} - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "def train_and_log_model(model, model_name, params, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_params(params)\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_test, y_test, model_name)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        return model, metrics\n",
    "\n",
    "def train_all_models(X_train, X_test, y_train, y_test):\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    mlflow.set_experiment(\"Tourism_Package_Prediction\")\n",
    "    results = {}\n",
    "    \n",
    "    dt_params = {'max_depth': 10, 'min_samples_split': 20, 'min_samples_leaf': 10, 'random_state': 42}\n",
    "    dt_model, dt_metrics = train_and_log_model(DecisionTreeClassifier(**dt_params), \"Decision_Tree\", dt_params, X_train, X_test, y_train, y_test)\n",
    "    results['Decision_Tree'] = {'model': dt_model, 'metrics': dt_metrics}\n",
    "    \n",
    "    rf_params = {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 5, 'random_state': 42, 'n_jobs': -1}\n",
    "    rf_model, rf_metrics = train_and_log_model(RandomForestClassifier(**rf_params), \"Random_Forest\", rf_params, X_train, X_test, y_train, y_test)\n",
    "    results['Random_Forest'] = {'model': rf_model, 'metrics': rf_metrics}\n",
    "    \n",
    "    ada_params = {'n_estimators': 100, 'learning_rate': 0.1, 'random_state': 42}\n",
    "    ada_model, ada_metrics = train_and_log_model(AdaBoostClassifier(**ada_params), \"AdaBoost\", ada_params, X_train, X_test, y_train, y_test)\n",
    "    results['AdaBoost'] = {'model': ada_model, 'metrics': ada_metrics}\n",
    "    \n",
    "    gb_params = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 5, 'random_state': 42}\n",
    "    gb_model, gb_metrics = train_and_log_model(GradientBoostingClassifier(**gb_params), \"Gradient_Boosting\", gb_params, X_train, X_test, y_train, y_test)\n",
    "    results['Gradient_Boosting'] = {'model': gb_model, 'metrics': gb_metrics}\n",
    "    \n",
    "    xgb_params = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42, 'use_label_encoder': False, 'eval_metric': 'logloss'}\n",
    "    xgb_model, xgb_metrics = train_and_log_model(XGBClassifier(**xgb_params), \"XGBoost\", xgb_params, X_train, X_test, y_train, y_test)\n",
    "    results['XGBoost'] = {'model': xgb_model, 'metrics': xgb_metrics}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def select_best_model(results):\n",
    "    comparison = []\n",
    "    for model_name, result in results.items():\n",
    "        metrics = result['metrics']\n",
    "        comparison.append({'Model': model_name, 'F1-Score': metrics['f1_score']})\n",
    "    comparison_df = pd.DataFrame(comparison).sort_values('F1-Score', ascending=False)\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"Best Model: {best_model_name}\")\n",
    "    return best_model, best_model_name\n",
    "\n",
    "def save_and_upload_model(model, model_name):\n",
    "    os.makedirs(\"tourism_project/model_building\", exist_ok=True)\n",
    "    model_path = f\"tourism_project/model_building/{model_name}_model.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "    api = HfApi()\n",
    "    repo_id = \"tourism-package-prediction-model\"\n",
    "    api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True, private=False)\n",
    "    api.upload_file(path_or_fileobj=model_path, path_in_repo=f\"{model_name}_model.pkl\", repo_id=repo_id, repo_type=\"model\")\n",
    "    print(f\"Model uploaded to {repo_id}\")\n",
    "\n",
    "def main():\n",
    "    print(\"MODEL TRAINING PIPELINE\")\n",
    "    X_train, X_test, y_train, y_test = load_train_test_data()\n",
    "    results = train_all_models(X_train, X_test, y_train, y_test)\n",
    "    best_model, best_model_name = select_best_model(results)\n",
    "    save_and_upload_model(best_model, best_model_name)\n",
    "    print(\"MODEL TRAINING COMPLETED\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tourism_project/deployment\", exist_ok=True)\n",
    "print(\"Created tourism_project/deployment folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/deployment/Dockerfile\n",
    "FROM python:3.9\n",
    "WORKDIR /app\n",
    "COPY . .\n",
    "RUN pip3 install -r requirements.txt\n",
    "RUN useradd -m -u 1000 user\n",
    "USER user\n",
    "ENV HOME=/home/user PATH=/home/user/.local/bin:$PATH\n",
    "WORKDIR $HOME/app\n",
    "COPY --chown=user . $HOME/app\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/deployment/app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "st.set_page_config(page_title=\"Tourism Package Predictor\", page_icon=\"‚úàÔ∏è\", layout=\"wide\")\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    try:\n",
    "        model_path = hf_hub_download(repo_id=\"tourism-package-prediction-model\", filename=\"XGBoost_model.pkl\", repo_type=\"model\")\n",
    "        return joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_input(data):\n",
    "    mappings = {\n",
    "        'TypeofContact': {'Company Invited': 0, 'Self Enquiry': 1},\n",
    "        'Occupation': {'Salaried': 0, 'Small Business': 1, 'Large Business': 2, 'Free Lancer': 3},\n",
    "        'Gender': {'Male': 0, 'Female': 1},\n",
    "        'ProductPitched': {'Basic': 0, 'Standard': 1, 'Deluxe': 2, 'Super Deluxe': 3, 'King': 4},\n",
    "        'MaritalStatus': {'Single': 0, 'Married': 1, 'Divorced': 2, 'Unmarried': 3},\n",
    "        'Designation': {'Executive': 0, 'Manager': 1, 'Senior Manager': 2, 'AVP': 3, 'VP': 4}\n",
    "    }\n",
    "    processed = data.copy()\n",
    "    for col, mapping in mappings.items():\n",
    "        if col in processed.columns:\n",
    "            processed[col] = processed[col].map(mapping)\n",
    "    return processed\n",
    "\n",
    "def main():\n",
    "    st.title(\"‚úàÔ∏è Tourism Package Prediction System\")\n",
    "    st.markdown(\"### Predict whether a customer will purchase the Wellness Tourism Package\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    model = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    st.sidebar.header(\"Input Method\")\n",
    "    input_method = st.sidebar.radio(\"Choose input method:\", [\"Manual Input\", \"CSV Upload\"])\n",
    "    \n",
    "    if input_method == \"Manual Input\":\n",
    "        st.header(\"üìù Enter Customer Details\")\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Demographics\")\n",
    "            age = st.number_input(\"Age\", 18, 100, 35)\n",
    "            gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "            marital_status = st.selectbox(\"Marital Status\", [\"Single\", \"Married\", \"Divorced\", \"Unmarried\"])\n",
    "            occupation = st.selectbox(\"Occupation\", [\"Salaried\", \"Small Business\", \"Large Business\", \"Free Lancer\"])\n",
    "            designation = st.selectbox(\"Designation\", [\"Executive\", \"Manager\", \"Senior Manager\", \"AVP\", \"VP\"])\n",
    "            monthly_income = st.number_input(\"Monthly Income\", 0, value=20000)\n",
    "        \n",
    "        with col2:\n",
    "            st.subheader(\"Travel Preferences\")\n",
    "            city_tier = st.selectbox(\"City Tier\", [1, 2, 3])\n",
    "            preferred_property_star = st.selectbox(\"Preferred Property Star\", [3.0, 4.0, 5.0])\n",
    "            number_of_person_visiting = st.number_input(\"Persons Visiting\", 1, 10, 2)\n",
    "            number_of_children_visiting = st.number_input(\"Children Visiting\", 0, 5, 0)\n",
    "            number_of_trips = st.number_input(\"Trips per Year\", 0.0, 20.0, 2.0)\n",
    "            passport = st.selectbox(\"Has Passport?\", [0, 1], format_func=lambda x: \"Yes\" if x == 1 else \"No\")\n",
    "            own_car = st.selectbox(\"Owns Car?\", [0, 1], format_func=lambda x: \"Yes\" if x == 1 else \"No\")\n",
    "        \n",
    "        with col3:\n",
    "            st.subheader(\"Interaction Details\")\n",
    "            type_of_contact = st.selectbox(\"Type of Contact\", [\"Company Invited\", \"Self Enquiry\"])\n",
    "            product_pitched = st.selectbox(\"Product Pitched\", [\"Basic\", \"Standard\", \"Deluxe\", \"Super Deluxe\", \"King\"])\n",
    "            duration_of_pitch = st.number_input(\"Duration of Pitch (min)\", 0.0, 60.0, 15.0)\n",
    "            number_of_followups = st.number_input(\"Follow-ups\", 0.0, 10.0, 3.0)\n",
    "            pitch_satisfaction_score = st.selectbox(\"Pitch Satisfaction\", [1, 2, 3, 4, 5])\n",
    "        \n",
    "        input_data = pd.DataFrame({\n",
    "            'Age': [age], 'TypeofContact': [type_of_contact], 'CityTier': [city_tier],\n",
    "            'DurationOfPitch': [duration_of_pitch], 'Occupation': [occupation], 'Gender': [gender],\n",
    "            'NumberOfPersonVisiting': [number_of_person_visiting], 'NumberOfFollowups': [number_of_followups],\n",
    "            'ProductPitched': [product_pitched], 'PreferredPropertyStar': [preferred_property_star],\n",
    "            'MaritalStatus': [marital_status], 'NumberOfTrips': [number_of_trips], 'Passport': [passport],\n",
    "            'PitchSatisfactionScore': [pitch_satisfaction_score], 'OwnCar': [own_car],\n",
    "            'NumberOfChildrenVisiting': [number_of_children_visiting], 'Designation': [designation],\n",
    "            'MonthlyIncome': [monthly_income]\n",
    "        })\n",
    "        \n",
    "        if st.button(\"üîÆ Predict\", type=\"primary\"):\n",
    "            processed_data = preprocess_input(input_data)\n",
    "            prediction = model.predict(processed_data)[0]\n",
    "            prediction_proba = model.predict_proba(processed_data)[0]\n",
    "            st.markdown(\"---\")\n",
    "            st.header(\"üìä Prediction Results\")\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                if prediction == 1:\n",
    "                    st.success(\"‚úÖ Customer is likely to purchase!\")\n",
    "                else:\n",
    "                    st.error(\"‚ùå Customer is unlikely to purchase.\")\n",
    "            with col2:\n",
    "                st.metric(\"Confidence\", f\"{max(prediction_proba)*100:.2f}%\")\n",
    "    \n",
    "    else:\n",
    "        st.header(\"üìÅ Upload CSV File\")\n",
    "        uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "        if uploaded_file is not None:\n",
    "            df = pd.read_csv(uploaded_file)\n",
    "            st.dataframe(df.head())\n",
    "            if st.button(\"üîÆ Predict All\", type=\"primary\"):\n",
    "                processed_data = preprocess_input(df)\n",
    "                predictions = model.predict(processed_data)\n",
    "                df['Prediction'] = predictions\n",
    "                df['Prediction_Label'] = df['Prediction'].map({0: 'Will Not Purchase', 1: 'Will Purchase'})\n",
    "                st.header(\"üìä Results\")\n",
    "                st.metric(\"Total\", len(df))\n",
    "                st.metric(\"Likely to Purchase\", (predictions == 1).sum())\n",
    "                st.dataframe(df[['Prediction_Label']])\n",
    "                st.download_button(\"üì• Download Results\", df.to_csv(index=False), \"predictions.csv\", \"text/csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/deployment/requirements.txt\n",
    "streamlit==1.43.2\n",
    "pandas==2.2.2\n",
    "numpy==1.26.4\n",
    "scikit-learn==1.6.0\n",
    "xgboost==2.1.4\n",
    "joblib==1.5.1\n",
    "huggingface-hub==0.32.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/hosting.py\n",
    "import os\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "def push_to_huggingface_space():\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged in to Hugging Face\")\n",
    "    else:\n",
    "        print(\"Error: HF_TOKEN not found\")\n",
    "        return\n",
    "    \n",
    "    api = HfApi()\n",
    "    space_id = \"tourism-package-prediction-app\"\n",
    "    \n",
    "    try:\n",
    "        api.create_repo(repo_id=space_id, repo_type=\"space\", space_sdk=\"streamlit\", exist_ok=True, private=False)\n",
    "        print(f\"Space '{space_id}' created/verified\")\n",
    "        \n",
    "        files = [\n",
    "            (\"tourism_project/deployment/Dockerfile\", \"Dockerfile\"),\n",
    "            (\"tourism_project/deployment/app.py\", \"app.py\"),\n",
    "            (\"tourism_project/deployment/requirements.txt\", \"requirements.txt\")\n",
    "        ]\n",
    "        \n",
    "        for local_path, repo_path in files:\n",
    "            if os.path.exists(local_path):\n",
    "                api.upload_file(path_or_fileobj=local_path, path_in_repo=repo_path, repo_id=space_id, repo_type=\"space\")\n",
    "                print(f\"Uploaded {repo_path}\")\n",
    "        \n",
    "        print(f\"All files uploaded to {space_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    push_to_huggingface_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Actions Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tourism_project/.github/workflows\", exist_ok=True)\n",
    "print(\"Created tourism_project/.github/workflows folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/.github/workflows/pipeline.yml\n",
    "name: MLOps Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  register-dataset:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r tourism_project/requirements.txt\n",
    "      - name: Upload Dataset to Hugging Face\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python tourism_project/data_registration.py\n",
    "\n",
    "  data-prep:\n",
    "    needs: register-dataset\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r tourism_project/requirements.txt\n",
    "      - name: Run Data Preparation\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python tourism_project/data_preparation.py\n",
    "\n",
    "  model-training:\n",
    "    needs: data-prep\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r tourism_project/requirements.txt\n",
    "      - name: Start MLflow Server\n",
    "        run: |\n",
    "          nohup mlflow ui --host 0.0.0.0 --port 5000 &\n",
    "          sleep 5\n",
    "      - name: Model Training\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python tourism_project/model_building/model_training.py\n",
    "\n",
    "  deploy-hosting:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [model-training, data-prep, register-dataset]\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Install Dependencies\n",
    "        run: pip install -r tourism_project/requirements.txt\n",
    "      - name: Push to Hugging Face Space\n",
    "        env:\n",
    "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "        run: python tourism_project/hosting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tourism_project/requirements.txt\n",
    "huggingface-hub==0.32.6\n",
    "pandas==2.2.2\n",
    "numpy==1.26.4\n",
    "scikit-learn==1.6.0\n",
    "xgboost==2.1.4\n",
    "mlflow==3.0.1\n",
    "joblib==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook contains all the necessary code to build a complete MLOps pipeline for Tourism Package Prediction:\n",
    "\n",
    "1. **Data Registration**: Uploads tourism.csv to Hugging Face Dataset Hub\n",
    "2. **Data Preparation**: Cleans, encodes, and splits data into train/test sets\n",
    "3. **Model Training**: Trains 5 models (Decision Tree, Random Forest, AdaBoost, Gradient Boosting, XGBoost) with MLflow tracking\n",
    "4. **Deployment**: Creates Streamlit app with Dockerfile and requirements\n",
    "5. **Hosting**: Pushes deployment files to Hugging Face Space\n",
    "6. **CI/CD**: GitHub Actions workflow automates the entire pipeline\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. Copy the `tourism.csv` file to `tourism_project/data/` folder\n",
    "2. Push the `tourism_project` folder to your GitHub repository\n",
    "3. The GitHub Actions workflow will automatically execute all pipeline steps\n",
    "4. Access your deployed app on Hugging Face Spaces\n",
    "\n",
    "**Note**: Remember to replace placeholder Hugging Face user IDs in the scripts with your actual Hugging Face username before deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
